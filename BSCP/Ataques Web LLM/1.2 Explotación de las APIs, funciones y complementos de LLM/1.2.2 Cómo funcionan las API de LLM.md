
---

### 🔧 ¿Cómo funcionan las APIs en LLMs? 

Cuando un LLM se integra con una API externa, puede seguir un flujo estructurado para que el cliente (sitio web o app) interprete y ejecute funciones solicitadas por el modelo.

---

### 🧭 Flujo típico de integración:

1. 💬 El **usuario envía un prompt** al LLM.
    
2. 🤖 El LLM detecta que debe usarse una función y **genera un JSON** con los argumentos requeridos.
    
3. 🔁 El **cliente llama a la función/API** con esos argumentos.
    
4. 📥 El cliente **recibe la respuesta** de la función.
    
5. 📤 El cliente **reenvía la respuesta al LLM** como nuevo mensaje.
    
6. 🔗 El LLM usa la respuesta para completar la interacción o **hacer otra llamada API interna**.
    
7. 📝 Finalmente, el LLM **resume y muestra el resultado** al usuario.
    

---

### ⚠️ Riesgos de seguridad

- El LLM **actúa como intermediario entre el usuario y APIs sensibles**.
    
- El usuario puede **no saber que se están realizando llamadas reales** a funciones backend.
    
- 🔐 Se recomienda implementar una **etapa de confirmación** antes de ejecutar cualquier API crítica.
    

---
