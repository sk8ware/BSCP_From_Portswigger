
---

### 🧨 Ataques LLM y Prompt Injection 

Muchos ataques a modelos de lenguaje en aplicaciones web se basan en una técnica llamada **prompt injection**.

---

### ⚙️ ¿Qué es prompt injection?

Es cuando un atacante envía un **prompt manipulado** para alterar el comportamiento del modelo y lograr que:

- 🛑 Rompa sus restricciones o políticas de seguridad.
    
- 🔐 Realice acciones **no autorizadas** como llamadas a APIs sensibles.
    
- 🧾 Genere respuestas **incorrectas o maliciosas**, fuera del objetivo original del sistema.
    

---

### 🎯 Objetivo del atacante

Engañar al LLM para que **desobedezca sus instrucciones predefinidas** y actúe en favor del atacante, utilizando solo texto manipulado.

---
