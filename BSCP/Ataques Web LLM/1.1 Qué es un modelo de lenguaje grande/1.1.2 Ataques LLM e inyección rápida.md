
---

### ğŸ§¨ Ataques LLM y Prompt Injection 

Muchos ataques a modelos de lenguaje en aplicaciones web se basan en una tÃ©cnica llamada **prompt injection**.

---

### âš™ï¸ Â¿QuÃ© es prompt injection?

Es cuando un atacante envÃ­a un **prompt manipulado** para alterar el comportamiento del modelo y lograr que:

- ğŸ›‘ Rompa sus restricciones o polÃ­ticas de seguridad.
    
- ğŸ” Realice acciones **no autorizadas** como llamadas a APIs sensibles.
    
- ğŸ§¾ Genere respuestas **incorrectas o maliciosas**, fuera del objetivo original del sistema.
    

---

### ğŸ¯ Objetivo del atacante

EngaÃ±ar al LLM para que **desobedezca sus instrucciones predefinidas** y actÃºe en favor del atacante, utilizando solo texto manipulado.

---
