
---

### ğŸ” DetecciÃ³n de vulnerabilidades en LLM

Para identificar vulnerabilidades en modelos de lenguaje grandes (LLM), se recomienda seguir una **metodologÃ­a estructurada**:

---

### ğŸ§­ MetodologÃ­a recomendada:

1. **Identificar los puntos de entrada del LLM:**
    
    - ğŸ—£ï¸ Entradas directas â†’ como prompts enviados por usuarios.
        
    - ğŸ§  Entradas indirectas â†’ como datos usados para entrenar o configurar el modelo.
        
2. **Determinar los recursos a los que tiene acceso el modelo:**
    
    - ğŸ§¾ Datos sensibles (como informaciÃ³n del usuario).
        
    - ğŸ”— Conexiones a APIs internas o servicios backend.
        
3. **Explorar esta nueva superficie de ataque:**
    
    - ğŸ’£ Intenta identificar vectores como prompt injection, fugas de datos o mal uso de funciones API.
        
    - ğŸ§ª Aplica pruebas similares a un pentest, pero adaptadas al flujo conversacional.
        

---

