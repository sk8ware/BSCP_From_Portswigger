
---

### 🔎 Detección de vulnerabilidades en LLM

Para identificar vulnerabilidades en modelos de lenguaje grandes (LLM), se recomienda seguir una **metodología estructurada**:

---

### 🧭 Metodología recomendada:

1. **Identificar los puntos de entrada del LLM:**
    
    - 🗣️ Entradas directas → como prompts enviados por usuarios.
        
    - 🧠 Entradas indirectas → como datos usados para entrenar o configurar el modelo.
        
2. **Determinar los recursos a los que tiene acceso el modelo:**
    
    - 🧾 Datos sensibles (como información del usuario).
        
    - 🔗 Conexiones a APIs internas o servicios backend.
        
3. **Explorar esta nueva superficie de ataque:**
    
    - 💣 Intenta identificar vectores como prompt injection, fugas de datos o mal uso de funciones API.
        
    - 🧪 Aplica pruebas similares a un pentest, pero adaptadas al flujo conversacional.
        

---

